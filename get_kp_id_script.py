import os
import re
import json
import glob
import asyncio
from urllib.parse import unquote
from ddgs import DDGS
from bs4 import BeautifulSoup
import requests

DEBUG = True

def dbg(*args):
    # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å [SEARCH]
    try:
        msg = ' '.join(str(a) for a in args)
        if msg.startswith('[SEARCH]'):
            print(msg)
    except Exception:
        pass

def normalize_premiere_date(date_str: str) -> str | None:
    if not date_str:
        return None
    s = date_str.strip()

    months_gen = {
        1: '—è–Ω–≤–∞—Ä—è', 2: '—Ñ–µ–≤—Ä–∞–ª—è', 3: '–º–∞—Ä—Ç–∞', 4: '–∞–ø—Ä–µ–ª—è',
        5: '–º–∞—è', 6: '–∏—é–Ω—è', 7: '–∏—é–ª—è', 8: '–∞–≤–≥—É—Å—Ç–∞',
        9: '—Å–µ–Ω—Ç—è–±—Ä—è', 10: '–æ–∫—Ç—è–±—Ä—è', 11: '–Ω–æ—è–±—Ä—è', 12: '–¥–µ–∫–∞–±—Ä—è',
    }
    # –í–∞—Ä–∏–∞–Ω—Ç—ã —Ä—É—Å—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π (—Ä–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π/–∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π/—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è)
    month_name_to_num = {
        '—è–Ω–≤–∞—Ä—è':1, '—è–Ω–≤–∞—Ä—å':1, '—è–Ω–≤':1,
        '—Ñ–µ–≤—Ä–∞–ª—è':2, '—Ñ–µ–≤—Ä–∞–ª—å':2, '—Ñ–µ–≤':2,
        '–º–∞—Ä—Ç–∞':3, '–º–∞—Ä—Ç':3, '–º–∞—Ä':3,
        '–∞–ø—Ä–µ–ª—è':4, '–∞–ø—Ä–µ–ª—å':4, '–∞–ø—Ä':4,
        '–º–∞—è':5, '–º–∞–π':5,
        '–∏—é–Ω—è':6, '–∏—é–Ω—å':6, '–∏—é–Ω':6,
        '–∏—é–ª—è':7, '–∏—é–ª—å':7, '–∏—é–ª':7,
        '–∞–≤–≥—É—Å—Ç–∞':8, '–∞–≤–≥—É—Å—Ç':8, '–∞–≤–≥':8,
        '—Å–µ–Ω—Ç—è–±—Ä—è':9, '—Å–µ–Ω—Ç—è–±—Ä—å':9, '—Å–µ–Ω':9, '—Å–µ–Ω—Ç':9,
        '–æ–∫—Ç—è–±—Ä—è':10, '–æ–∫—Ç—è–±—Ä—å':10, '–æ–∫—Ç':10,
        '–Ω–æ—è–±—Ä—è':11, '–Ω–æ—è–±—Ä—å':11, '–Ω–æ—è':11, '–Ω–æ—è–±':11,
        '–¥–µ–∫–∞–±—Ä—è':12, '–¥–µ–∫–∞–±—Ä—å':12, '–¥–µ–∫':12,
    }

    low = s.lower()

    # 1) YYYY-MM-DD –∏–ª–∏ YYYY/MM/DD –∏–ª–∏ YYYY.MM.DD
    m = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', low)
    if m:
        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
        if 1 <= mo <= 12 and 1 <= d <= 31:
            return f'{d} {months_gen.get(mo, str(mo))} {y}'

    # 2) DD-MM-YYYY –∏–ª–∏ DD/MM/YYYY –∏–ª–∏ DD.MM.YYYY
    m = re.search(r'(\d{1,2})[./-](\d{1,2})[./-](\d{4})', low)
    if m:
        d, mo, y = int(m.group(1)), int(m.group(2)), int(m.group(3))
        if 1 <= mo <= 12 and 1 <= d <= 31:
            return f'{d} {months_gen.get(mo, str(mo))} {y}'

    # 3) DD <–º–µ—Å—è—Ü-—Å–ª–æ–≤–æ–º> YYYY (–ª—é–±–∞—è —Ñ–æ—Ä–º–∞ –º–µ—Å—è—Ü–∞)
    m = re.search(r'(\d{1,2})\s+([–ê-–Ø–∞-—è–Å—ë]+)\s+(\d{4})', low)
    if m:
        d, mon_name, y = int(m.group(1)), m.group(2).lower(), int(m.group(3))
        mo = month_name_to_num.get(mon_name)
        if mo and 1 <= d <= 31:
            return f'{d} {months_gen[mo]} {y}'
        # –µ—Å–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª–∏ –º–µ—Å—è—Ü ‚Äî –≤–µ—Ä–Ω—ë–º –∫–∞–∫ –µ—Å—Ç—å, –Ω–æ –ø–æ–¥—á–∏—Å—Ç–∏–º –ø—Ä–æ–±–µ–ª—ã/—Ä–µ–≥–∏—Å—Ç—Ä
        return f'{d} {mon_name} {y}'

    # –ù–∏—á–µ–≥–æ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª–∏ ‚Äî –≤–µ—Ä–Ω—ë–º –∏—Å—Ö–æ–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É
    return s

# –û–¥–∏–Ω –∫–ª–∏–µ–Ω—Ç DDG; –¥–æ–±–∞–≤–∏–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
ddgs_client = DDGS(timeout=6)

def _clean_title(s: str) -> str:
    s = (s or '').strip()
    if not s:
        return ''
    m = re.match(r'^(.*?)\s*\(', s)
    return (m.group(1) if m else s).strip()

def _text(el):
    return el.get_text(' ', strip=True) if el else ''

def _first_a_text(li):
    a = li.find('a')
    return a.get_text(strip=True) if a else ''

def _first_a_href(li):
    a = li.find('a')
    return a['href'] if a and a.has_attr('href') else ''

def _norm_category(href: str, text: str) -> str | None:
    href = (href or '').lower()
    text = (text or '').lower()
    for key in ('/filmy/', 'filmy'):
        if key in href or key in text:
            return 'filmy'
    for key in ('/serialy/', 'serialy'):
        if key in href or key in text:
            return 'serialy'
    for key in ('/mult', '/multfilmy/', '–º—É–ª—å—Ç', '–º—É–ª—å—Ç–∏', '–º—É–ª—å—Ç—Ñ–∏–ª—å'):
        if key in href or key in text:
            return 'multfilmy'
    for key in ('/anime/', 'anime', '–∞–Ω–∏–º–µ'):
        if key in href or key in text:
            return 'anime'
    return None

def _parse_ratings(soup: BeautifulSoup) -> tuple[float | None, float | None]:
    # –ò—â–µ–º li —Å –ø–æ–¥–ø–∏—Å—å—é "–†–µ–π—Ç–∏–Ω–≥–∏:"
    kp = None
    imdb = None
    for li in soup.select('ul.pmovie__list li'):
        t = _text(li)
        if t.startswith('–†–µ–π—Ç–∏–Ω–≥–∏:'):
            # –ü—Ä–∏–º–µ—Ä—ã: "IMDb: 9.3 (63), " –∏–ª–∏ "–ö–∏–Ω–æ–ü–æ–∏—Å–∫: 7.1"
            m_imdb = re.search(r'IMDb:\s*([0-9]+(?:[.,][0-9])?)', t, re.I)
            if m_imdb:
                imdb = float(m_imdb.group(1).replace(',', '.'))
            m_kp = re.search(r'(–ö–∏–Ω–æ–ü–æ–∏—Å–∫|–ö–ü):\s*([0-9]+(?:[.,][0-9])?)', t, re.I)
            if m_kp:
                kp = float(m_kp.group(2).replace(',', '.'))
            break
    return kp, imdb

def _extract_trailer(soup: BeautifulSoup) -> tuple[str | None, str | None]:
    # iframe —Ç—Ä–µ–π–ª–µ—Ä–∞ –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∞ id="trailer-block"
    iframe = soup.select_one('#trailer-block iframe')
    if not iframe:
        return None, None
    src = iframe.get('data-src') or iframe.get('src')
    if not src:
        return None, None
    src = unquote(src)
    # youtubeId –∏–∑ /embed/<ID>
    m = re.search(r'/embed/([\w\-]{5,})', src)
    if not m:
        return None, src
    yid = m.group(1)
    # –°—Ñ–æ—Ä–º–∏—Ä—É–µ–º watch-—Å—Å—ã–ª–∫—É
    trailer_url = f'https://www.youtube.com/watch?v={yid}'
    return yid, trailer_url

def parse_movie_html(file_path: str) -> dict | None:
    """
    –ü–∞—Ä—Å–∏—Ç –≤—Å–µ –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è –∏–∑ HTML –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        soup = BeautifulSoup(content, 'lxml')

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ (+ —Ñ–æ–ª–ª–±–µ–∫–∏)
        h1 = soup.find('h1')
        title = _clean_title(h1.get_text(strip=True) if h1 else '')

        if not title:
            img = soup.select_one('.pmovie__img img')
            if img and img.has_attr('alt'):
                title = _clean_title(img['alt'])

        if not title:
            meta = soup.find('meta', attrs={'property': 'og:title'}) or soup.find('meta', attrs={'name': 'og:title'})
            if meta and meta.get('content'):
                title = _clean_title(meta['content'])

        if not title and soup.title:
            doc_title = soup.title.get_text(strip=True)
            # —á–∞—Å—Ç–∞—è —Ñ–æ—Ä–º–∞: "<–∏–º—è> ‚Äî —Å–º–æ—Ç—Ä–µ—Ç—å..." ‚Üí –±–µ—Ä—ë–º —Ç–æ, —á—Ç–æ –¥–æ —Ç–∏—Ä–µ/–¥–µ—Ñ–∏—Å–∞
            doc_title = re.split(r'\s+[‚Äì‚Äî-]\s+', doc_title, 1)[0]
            title = _clean_title(doc_title)

        # –ì–æ–¥ –∏–∑ "pmovie__main-info"
        year = None
        info_div = soup.find('div', class_='pmovie__main-info')
        if info_div:
            txt = _text(info_div)
            ym = re.search(r'\b(19|20)\d{2}\b', txt)
            if ym:
                year = int(ym.group(0))

        # –§–æ–ª–ª–±–µ–∫: –∏–∑ —Å—Ç—Ä–æ–∫–∏ "–ì–æ–¥:"
        if year is None:
            for li in soup.select('ul.pmovie__list li'):
                span = li.find('span')
                if span and '–ì–æ–¥:' in span.get_text(strip=True):
                    txt = _text(li).replace('–ì–æ–¥:', '').strip()
                    ym2 = re.search(r'\b(19|20)\d{2}\b', txt)
                    if ym2:
                        year = int(ym2.group(0))
                        break

        # –°–µ–∑–æ–Ω: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∏–∑ —à–∞–ø–∫–∏ (h1 / og:title / <title>), –Ω–µ –∏–∑ .poster__series
        season = None
        header_sources = []
        if h1 and h1.get_text(strip=True):
            header_sources.append(h1.get_text(strip=True))
        meta = soup.find('meta', attrs={'property': 'og:title'}) or soup.find('meta', attrs={'name': 'og:title'})
        if meta and meta.get('content'):
            header_sources.append(meta['content'])
        if soup.title and soup.title.string:
            header_sources.append(soup.title.string)

        season_match = None
        for hs in header_sources:
            m_season = re.search(r'\b(\d+)\s+—Å–µ–∑–æ–Ω\b', hs, re.I)
            if m_season:
                season_match = m_season.group(0)
                break
        if season_match:
            season = season_match

        # –ö–∞—Ä—Ç–∏–Ω–∫–∞ –ø–æ—Å—Ç–µ—Ä–∞
        img = soup.select_one('.pmovie__img img')
        image = None
        original_title = None
        if img and img.has_attr('src'):
            image = img['src'].lstrip('/')
        # –ü–æ–ø—ã—Ç–∫–∞ –≤—ã—Ç–∞—â–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ alt (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–∫–æ–±–∫–∏, –Ω–µ –≥–æ–¥)
        if img and img.has_attr('alt'):
            alt = img['alt']
            # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ —Å–∫–æ–±–∫–∞—Ö, –µ—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ –Ω–µ —Ç–æ–ª—å–∫–æ –≥–æ–¥
            parens = re.findall(r'\(([^)]+)\)', alt)
            if parens:
                candidate = parens[-1]
                if not re.fullmatch(r'(19|20)\d{2}', candidate):
                    original_title = candidate.strip()

        # –û–ø–∏—Å–∞–Ω–∏–µ: —Å–æ—Ö—Ä–∞–Ω—è–µ–º HTML/–ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        descr_div = soup.select_one('.pmovie__text.full-text.clearfix')
        description = descr_div.decode_contents() if descr_div else None

        # –°—Ç—Ä–∞–Ω–∞
        country = None
        for li in soup.select('ul.pmovie__list li'):
            span = li.find('span')
            if span and '–°—Ç—Ä–∞–Ω–∞:' in span.get_text(strip=True):
                country = _text(li).replace('–°—Ç—Ä–∞–Ω–∞:', '').strip()
                break

        # –ü—Ä–µ–º—å–µ—Ä–∞ (–ú–∏—Ä)
        premiere = None
        for li in soup.select('ul.pmovie__list li'):
            span = li.find('span')
            if span and '–ü—Ä–µ–º—å–µ—Ä–∞ (–ú–∏—Ä)' in span.get_text(strip=True):
                premiere = _text(li).replace('–ü—Ä–µ–º—å–µ—Ä–∞ (–ú–∏—Ä)', '').strip()
                break

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞—Ç—ã –ø—Ä–µ–º—å–µ—Ä—ã –∫ —Ñ–æ—Ä–º–∞—Ç—É "12 —Ñ–µ–≤—Ä–∞–ª—è 2011"
        premiere = normalize_premiere_date(premiere) if premiere else None

        # –§–æ–ª–ª–±–µ–∫: –≥–æ–¥ –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–µ–º—å–µ—Ä—ã
        if year is None and premiere:
            ym_p = re.search(r'\b(19|20)\d{2}\b', premiere)
            if ym_p:
                year = int(ym_p.group(0))

        # –§–æ–ª–ª–±–µ–∫: –≥–æ–¥ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (–±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π 4-–∑–Ω–∞—á–Ω—ã–π –≥–æ–¥)
        if year is None:
            base = os.path.basename(file_path)
            years_in_name = re.findall(r'(19|20)\d{2}', base)
            if years_in_name:
                year = int(years_in_name[-1])

        # –†–µ–∂–∏—Å—Å–µ—Ä (–ø–µ—Ä–≤—ã–π)
        director = None
        for li in soup.select('ul.pmovie__list li'):
            span = li.find('span')
            if span and '–†–µ–∂–∏—Å—Å–µ—Ä:' in span.get_text(strip=True):
                director = _first_a_text(li) or _text(li).replace('–†–µ–∂–∏—Å—Å–µ—Ä:', '').split(',')[0].strip()
                break

        # –ê–∫—Ç–µ—Ä—ã (—Å–ø–∏—Å–∫–æ–º –∏ —Å—Ç—Ä–æ–∫–æ–π)
        actors_list = []
        for li in soup.select('ul.pmovie__list li'):
            span = li.find('span')
            if span and '–í —Ä–æ–ª—è—Ö –∞–∫—Ç–µ—Ä—ã:' in span.get_text(strip=True):
                actors_list = [a.get_text(strip=True) for a in li.find_all('a')]
                break
        actors = ', '.join(actors_list) if actors_list else None

        # –ñ–∞–Ω—Ä—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è
        genres = []
        category_norm = None
        for li in soup.select('ul.pmovie__list li'):
            span = li.find('span')
            if span and '–ñ–∞–Ω—Ä:' in span.get_text(strip=True):
                anchors = li.find_all('a')
                if anchors:
                    cat_text = anchors[0].get_text(strip=True)
                    cat_href = anchors[0].get('href', '')
                    category_norm = _norm_category(cat_href, cat_text)
                    # –í JSON –∂–∞–Ω—Ä—ã —Å–æ–¥–µ—Ä–∂–∞—Ç –∏ –ø–µ—Ä–≤—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                    genres = [a.get_text(strip=True) for a in anchors]
                break

        # –ü–µ—Ä–µ–≤–æ–¥
        translation = None
        for li in soup.select('ul.pmovie__list li'):
            span = li.find('span')
            if span and '–ü–µ—Ä–µ–≤–æ–¥:' in span.get_text(strip=True):
                translation = _text(li).replace('–ü–µ—Ä–µ–≤–æ–¥:', '').strip()
                break

        # –¢—Ä–µ–π–ª–µ—Ä / youtube
        youtube_id, trailer = _extract_trailer(soup)

        # –†–µ–π—Ç–∏–Ω–≥–∏
        kp_rating, imdb_rating = _parse_ratings(soup)

        # ID (–ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞)
        file_id = os.path.splitext(os.path.basename(file_path))[0]

        return {
            'file_id': file_id,
            'category': category_norm,
            'title': title or None,
            'year': year,
            'season': season,
            'image': image,
            'description': description,  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º HTML/–ø–µ—Ä–µ–Ω–æ—Å—ã
            'originalTitle': original_title,
            'country': country,
            'premiere': premiere,
            'director': director,
            'genres': genres,
            'translation': translation,
            'actors': actors,
            'youtubeId': youtube_id,
            'trailer': trailer,
            'kpRating': kp_rating,
            'imdbRating': imdb_rating,
            'ageRating': None,
            'comments': [],
        }
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return None

def build_stage_sequence(info: dict) -> list[str]:
    stages = []
    if info.get('director'):
        stages.append('director')
    if info.get('actors'):
        first_actor = info['actors'].split(',')[0].strip()
        if first_actor:
            stages.append('actor')
    if info.get('premiere'):
        stages.append('premiere')
    return stages

def make_query(info: dict, mode: str, is_series: bool) -> str:
    title = info.get('title') or ''
    year = str(info.get('year') or '')
    parts = [f'"{title}"', f'"{year}"']
    if mode == 'director' and info.get('director'):
        parts.append(f'"{info["director"]}"')
    elif mode == 'actor' and info.get('actors'):
        first_actor = info['actors'].split(',')[0].strip()
        if first_actor:
            parts.append(f'"{first_actor}"')
    elif mode == 'premiere' and info.get('premiere'):
        parts.append(f'"{info["premiere"]}"')
    if is_series:
        parts.append('"—Å–µ–∑–æ–Ω"')
    return ' '.join(parts)

def _norm(s: str) -> str:
    s = (s or '').lower()
    s = s.replace('—ë', '–µ')
    s = re.sub(r'["‚Äú‚Äù¬´¬ª\'`]', '', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

TOKENS_OK = 0.7

def _tokenize(s: str) -> list[str]:
    txt = _norm(s)
    toks = re.findall(r'[a-z–∞-—è—ë0-9]+', txt, flags=re.I)
    return [t for t in toks if len(t) >= 3]

def _tokens_score(expected: str, candidate: str) -> float:
    exp = set(_tokenize(expected))
    if not exp:
        return 0.0
    cand = set(_tokenize(candidate))
    return len(exp & cand) / len(exp)

def _is_series(info: dict) -> bool:
    # –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    file_id = (info.get('file_id') or '').lower()
    if '-serial-' in file_id:
        return True
    # –ø–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    if (info.get('category') or '').lower() == 'serialy':
        return True
    # –ø–æ —è–≤–Ω–æ–º—É —Å–µ–∑–æ–Ω—É, –≤–∑—è—Ç–æ–º—É –∏–∑ —à–∞–ø–∫–∏ (h1/title/og:title)
    if info.get('season'):
        return True
    return False

async def get_kinopoisk_id_async(movie_query: str, expected_title: str, expected_year: int | str, mode: str, is_series: bool) -> tuple[str, str | None, str | None]:
    exp_title_norm = _norm(expected_title)
    year_str = str(expected_year or '').strip()

    quoted = re.findall(r'"([^"]+)"', movie_query)
    hint = quoted[2] if len(quoted) >= 3 else ''
    hint_norm = _norm(hint)

    site_filter = 'site:www.kinopoisk.ru/series/' if is_series else 'site:www.kinopoisk.ru/film/'
    dbg(f'[SEARCH] type={"series" if is_series else "film"} mode={mode} query={movie_query} expected="{expected_title}" year={year_str}')

    query = f'{movie_query} {site_filter} -site:hd.kinopoisk.ru -"—Å–º–æ—Ç—Ä–µ—Ç—å –æ–Ω–ª–∞–π–Ω –≤ —Ö–æ—Ä–æ—à–µ–º"'
    try:
        results = await asyncio.to_thread(
            ddgs_client.text, 
            query, 
            max_results=5,
            region='ru-ru',
            safesearch='off',
            backend='api',
        )
        any_candidates = False
        for result in results or []:
            url = result.get('href', '')
            title_field = result.get('title') or ''
            snippet = result.get('body') or ''
            title_norm = _norm(title_field)
            body_norm = _norm(snippet)

            m = re.search(r'kinopoisk\.ru/(film|series)/(\d+)', url)
            if not m:
                dbg(f'  [SKIP] not film/series page: {url}')
                continue

            url_type = m.group(1)
            if is_series and url_type != 'series':
                dbg(f'  [SKIP] expected series, got {url_type}: {url}')
                continue
            if (not is_series) and url_type != 'film':
                dbg(f'  [SKIP] expected film, got {url_type}: {url}')
                continue

            score_title = _tokens_score(expected_title, title_field)
            title_ok = (exp_title_norm in title_norm) or (score_title >= TOKENS_OK)
            year_ok = bool(year_str and (year_str in title_norm or year_str in body_norm))
            hint_ok = True if not hint_norm else (hint_norm in title_norm or hint_norm in body_norm or _tokens_score(hint, title_field) >= TOKENS_OK or _tokens_score(hint, snippet) >= TOKENS_OK)

            dbg(f'  [CANDIDATE] title="{title_field}" url={url} score_title={score_title:.2f} title_ok={title_ok} year_ok={year_ok} hint_ok={hint_ok}')
            if title_ok and year_ok:
                if hint_ok:
                    dbg(f'  [ACCEPT] id={m.group(2)} url={url}')
                else:
                    dbg(f'  [ACCEPT_NOHINT] id={m.group(2)} url={url}')
                return movie_query, m.group(2), url

            any_candidates = True

        if not any_candidates:
            dbg(f'[NO_RESULTS] mode={mode} query={movie_query}')
        else:
            dbg(f'[NO_MATCH] mode={mode} query={movie_query}')
        return movie_query, None, None
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ '{movie_query}': {e}")
        return movie_query, None, None

def build_json_item(info: dict, kinopoisk_id: str | None) -> dict:
    # category: –µ—Å–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª–∏ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –∏–∑ genres[0]
    category = info.get('category')
    if not category and info.get('genres'):
        category = _norm_category('', info['genres'][0]) or None

    return {
        'id': info['file_id'],
        'category': category,
        'title': info.get('title'),
        'year': info.get('year'),
        'season': info.get('season'),
        'image': info.get('image'),
        'description': info.get('description'),
        'originalTitle': info.get('originalTitle'),
        'country': info.get('country'),
        'premiere': info.get('premiere'),
        'director': info.get('director'),
        'genres': info.get('genres') or [],
        'translation': info.get('translation'),
        'actors': info.get('actors'),
        'kpRating': info.get('kpRating'),
        'imdbRating': info.get('imdbRating'),
        'youtubeId': info.get('youtubeId'),
        'trailer': info.get('trailer'),
        'kinopoiskId': kinopoisk_id,
        'ageRating': info.get('ageRating'),
        'comments': info.get('comments') or [],
    }

async def main():
    html_files = glob.glob('*.html')
    if not html_files:
        print("HTML —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.")
        return

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(html_files)} HTML —Ñ–∞–π–ª–æ–≤.")

    movies_info: list[dict] = []
    for file_path in html_files:
        info = parse_movie_html(file_path)
        # –¢—Ä–µ–±—É–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ title –∏ year –¥–ª—è –ø–æ–∏—Å–∫–∞ ID
        if info and info.get('title') and info.get('year'):
            movies_info.append(info)
        else:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –≥–æ–¥ –∏–∑ {os.path.basename(file_path)}. –ü—Ä–æ–ø—É—Å–∫.")

    if not movies_info:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∏–ª—å–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞.")
        return

    batch_size = 7
    total_batches = (len(movies_info) + batch_size - 1) // batch_size
    print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫ –¥–ª—è {len(movies_info)} —Ñ–∏–ª—å–º–æ–≤ –ø–æ—Ä—Ü–∏—è–º–∏ –ø–æ {batch_size}...")

    json_items: list[dict] = []

    for b in range(0, len(movies_info), batch_size):
        batch_items = movies_info[b:b + batch_size]
        per_item_stages = [build_stage_sequence(info) for info in batch_items]
        per_item_stage_index = [0 for _ in batch_items]
        per_item_attempts = [0 for _ in batch_items]
        batch_results: list[tuple[str, str | None, str | None]] = [('', None, None) for _ in batch_items]

        # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç–∞–¥–∏–∏ –Ω–µ—Ç (–Ω–µ—Ç —Ä–µ–∂–∏—Å—Å—ë—Ä–∞/–∞–∫—Ç—ë—Ä–∞/–ø—Ä–µ–º—å–µ—Ä—ã) ‚Äî —Å—Ä–∞–∑—É –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –ù–ï –ù–ê–ô–î–ï–ù –∏ –Ω–µ —à—Ç—É—Ä–º—É–µ–º
        for i, stages in enumerate(per_item_stages):
            if not stages:
                base_query = f'"{batch_items[i].get("title","")}" "{batch_items[i].get("year","")}"'
                batch_results[i] = (base_query, None, None)

        # –í —Ä–∞–±–æ—Ç—É –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Ç–µ, —É –∫–æ–≥–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ —Å—Ç–∞–¥–∏—è
        unresolved_idx = [i for i, stages in enumerate(per_item_stages) if stages]

        print(f"\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Ä—Ü–∏–∏ {b // batch_size + 1}/{total_batches} ({len(batch_items)} —Ñ–∏–ª—å–º–æ–≤) ---")

        # –ü–æ–∫–∞ —É –≤—Å–µ—Ö –Ω–µ—Ç ID ‚Äî –Ω–µ –¥–≤–∏–≥–∞–µ–º—Å—è –¥–∞–ª—å—à–µ
        while unresolved_idx:
            query_pack = []
            for i in unresolved_idx:
                mode = per_item_stages[i][per_item_stage_index[i]]
                is_series = _is_series(batch_items[i])
                q = make_query(batch_items[i], mode, is_series)
                query_pack.append((i, q, mode, is_series))

            tasks = [
                get_kinopoisk_id_async(
                    q,
                    batch_items[i].get('title', ''),
                    batch_items[i].get('year', ''),
                    mode,
                    is_series
                )
                for (i, q, mode, is_series) in query_pack
            ]
            results = await asyncio.gather(*tasks)
    
            still_unresolved = []
            for res, (i, q, mode, is_series) in zip(results, query_pack):
                movie_query, movie_id, source_url = res
                if movie_id:
                    batch_results[i] = (movie_query, movie_id, source_url)
                    dbg(f'[FOUND] mode={mode} -> {movie_id} {source_url}')
                else:
                    if per_item_stage_index[i] + 1 < len(per_item_stages[i]):
                        per_item_stage_index[i] += 1
                        still_unresolved.append(i)
                    else:
                        if per_item_attempts[i] == 0:
                            per_item_attempts[i] = 1
                            per_item_stage_index[i] = 0
                            still_unresolved.append(i)
                        else:
                            dbg(f'[GIVEUP] title="{batch_items[i].get("title","")}" year={batch_items[i].get("year","")} after 2 rounds')
                            batch_results[i] = (movie_query, None, None)

            unresolved_idx = still_unresolved
            if unresolved_idx:
                await asyncio.sleep(1)

        # –ü–µ—á–∞—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Ä—Ü–∏–∏
        print("\n–ü–æ—Ä—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–æ–π –ø–æ—Ä—Ü–∏–∏:")
        for movie_query, movie_id, source_url in batch_results:
            if movie_id:
                if source_url:
                    print(f"‚úÖ {movie_query} ‚Üí ID: {movie_id} | {source_url}")
                else:
                    print(f"‚úÖ {movie_query} ‚Üí ID: {movie_id}")
            else:
                print(f"‚ùå {movie_query} ‚Üí –ù–ï –ù–ê–ô–î–ï–ù")

        # –§–æ—Ä–º–∏—Ä—É–µ–º JSON –¥–ª—è —ç—Ç–æ–π –ø–æ—Ä—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ)
        for info, (_, kp_id, _) in zip(batch_items, batch_results):
            if kp_id:
                json_items.append(build_json_item(info, kp_id))

        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ—Ä—Ü–∏—è–º–∏
        if b + batch_size < len(movies_info):
            await asyncio.sleep(2)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π JSON –≤ —Ñ–∞–π–ª
    out_path = 'movies.json'
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(json_items, f, ensure_ascii=False, indent=2)

    print("\n" + "=" * 60)
    print(f"–ì–æ—Ç–æ–≤–æ. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(json_items)} –∑–∞–ø–∏—Å–µ–π –≤ {out_path}")
    print("=" * 60)

if __name__ == "__main__":
    # –í Windows –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã to_thread
    # asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())